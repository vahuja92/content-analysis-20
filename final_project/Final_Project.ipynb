{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud_2020 #pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
    "\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#These are all for the cluster detection\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "\n",
    "import scipy #For hierarchical clustering and some visuals\n",
    "#import scipy.cluster.hierarchy\n",
    "import gensim#For topic modeling\n",
    "import requests #For downloading our datasets\n",
    "import numpy as np #for arrays\n",
    "import pandas #gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import matplotlib.cm #Still for graphics\n",
    "import seaborn as sns #Makes the graphics look nicer\n",
    "\n",
    "# comp-linguistics\n",
    "import spacy\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning, it\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import zipfile\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting My Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using the COCA corpus, which I have downloaded to my local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"/Users/vedikaahuja/winter2020/Content-Analysis-2020/COCA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcorpus(corpus_name, corpus_style=\"text\"):\n",
    "    texts_raw = {}\n",
    "    for file in os.listdir(corpus_name + \"/\"):\n",
    "        if corpus_style in file:\n",
    "            print(file)\n",
    "            zfile = zipfile.ZipFile(corpus_name + \"/\" + file)\n",
    "            for file in zfile.namelist():\n",
    "                texts_raw[file] = []\n",
    "                with zfile.open(file) as f:\n",
    "                    for line in f:\n",
    "                        texts_raw[file].append(line)\n",
    "    return texts_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_magazine_jex.zip\n",
      "text_spoken_kde.zip\n",
      "text_fiction_awq.zip\n",
      "text_newspaper_lsp.zip\n",
      "text_academic_rpe.zip\n",
      "text_2012-2015_ksr.zip\n"
     ]
    }
   ],
   "source": [
    "coca_raw = loadcorpus(corpus_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_texts = random.sample(list(coca_raw.keys()), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_text(raw_texts):\n",
    "    clean_texts = []\n",
    "    for text in raw_texts:\n",
    "        try:\n",
    "            text = text.decode(\"utf-8\")\n",
    "            clean_text = text.replace(\" \\'m\", \"'m\").replace(\" \\'ll\", \"'ll\").replace(\" \\'re\", \"'re\").replace(\"\\'s\", \"'s\").replace(\" \\'re\", \"'re\").replace(\" n\\'t\", \"n't\").replace(\" \\'ve\", \"'ve\").replace(\" /'d\", \"'d\").replace(\"<p>\", \"\").replace(\"@\", \"\")\n",
    "            clean_texts.append(clean_text)\n",
    "        except AttributeError:\n",
    "            # print(\"ERROR CLEANING\")\n",
    "            # print(text)\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            # print(\"Unicode Error, Skip\")\n",
    "            continue\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = {\"year\":[] , \"genre\":[], \"text\":[], \"tokenized_text\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_mag_1990.txt\n",
      "w_mag_1991.txt\n",
      "w_mag_1992.txt\n",
      "w_mag_1993.txt\n",
      "w_mag_1994.txt\n",
      "w_mag_1995.txt\n",
      "w_mag_1997.txt\n",
      "w_mag_1998.txt\n",
      "w_mag_1999.txt\n",
      "w_mag_2000.txt\n",
      "w_mag_2001.txt\n",
      "w_mag_2002.txt\n",
      "w_mag_2003.txt\n",
      "w_mag_2004.txt\n",
      "w_mag_2005.txt\n",
      "w_mag_2006.txt\n",
      "w_mag_2007.txt\n",
      "w_mag_2008.txt\n",
      "w_mag_2009.txt\n",
      "w_mag_2010.txt\n",
      "w_mag_2011.txt\n",
      "w_mag_2012.txt\n",
      "w_mag_1996.txt\n",
      "w_spok_1990.txt\n",
      "w_spok_1991.txt\n",
      "w_spok_1992.txt\n",
      "w_spok_1993.txt\n",
      "w_spok_1994.txt\n",
      "w_spok_1995.txt\n",
      "w_spok_1996.txt\n",
      "w_spok_1997.txt\n",
      "w_spok_1998.txt\n",
      "w_spok_1999.txt\n",
      "w_spok_2000.txt\n",
      "w_spok_2001.txt\n",
      "w_spok_2002.txt\n",
      "w_spok_2003.txt\n",
      "w_spok_2004.txt\n",
      "w_spok_2005.txt\n",
      "w_spok_2006.txt\n",
      "w_spok_2007.txt\n",
      "w_spok_2008.txt\n",
      "w_spok_2009.txt\n",
      "w_spok_2010.txt\n",
      "w_spok_2011.txt\n",
      "w_spok_2012.txt\n",
      "w_fic_1990.txt\n",
      "w_fic_1991.txt\n",
      "w_fic_1992.txt\n",
      "w_fic_1993.txt\n",
      "w_fic_1994.txt\n",
      "w_fic_1995.txt\n",
      "w_fic_1996.txt\n",
      "w_fic_1997.txt\n",
      "w_fic_1998.txt\n",
      "w_fic_1999.txt\n",
      "w_fic_2000.txt\n",
      "w_fic_2001.txt\n",
      "w_fic_2002.txt\n",
      "w_fic_2003.txt\n",
      "w_fic_2004.txt\n",
      "w_fic_2005.txt\n",
      "w_fic_2006.txt\n",
      "w_fic_2007.txt\n",
      "w_fic_2008.txt\n",
      "w_fic_2009.txt\n",
      "w_fic_2010.txt\n",
      "w_fic_2011.txt\n",
      "w_fic_2012.txt\n",
      "w_news_1990.txt\n",
      "w_news_1991.txt\n",
      "w_news_1992.txt\n",
      "w_news_1993.txt\n",
      "w_news_1994.txt\n",
      "w_news_1995.txt\n",
      "w_news_1996.txt\n",
      "w_news_1997.txt\n",
      "w_news_1998.txt\n",
      "w_news_1999.txt\n",
      "w_news_2000.txt\n",
      "w_news_2001.txt\n",
      "w_news_2002.txt\n",
      "w_news_2003.txt\n",
      "w_news_2004.txt\n",
      "w_news_2005.txt\n",
      "w_news_2006.txt\n",
      "w_news_2007.txt\n",
      "w_news_2008.txt\n",
      "w_news_2009.txt\n",
      "w_news_2010.txt\n",
      "w_news_2011.txt\n",
      "w_news_2012.txt\n",
      "w_acad_1990.txt\n",
      "w_acad_1991.txt\n",
      "w_acad_1992.txt\n",
      "w_acad_1993.txt\n",
      "w_acad_1994.txt\n",
      "w_acad_1995.txt\n",
      "w_acad_1996.txt\n",
      "w_acad_1997.txt\n",
      "w_acad_1998.txt\n",
      "w_acad_1999.txt\n",
      "w_acad_2000.txt\n",
      "w_acad_2001.txt\n",
      "w_acad_2002.txt\n",
      "w_acad_2003.txt\n",
      "w_acad_2004.txt\n",
      "w_acad_2005.txt\n",
      "w_acad_2006.txt\n",
      "w_acad_2007.txt\n",
      "w_acad_2008.txt\n",
      "w_acad_2009.txt\n",
      "w_acad_2010.txt\n",
      "w_acad_2011.txt\n",
      "w_acad_2012.txt\n",
      "2013_mag.txt\n",
      "2013_news.txt\n",
      "2013_spok.txt\n",
      "2014_acad.txt\n",
      "2014_fic.txt\n",
      "2014_mag.txt\n",
      "2014_news.txt\n",
      "2014_spok.txt\n",
      "2015_acad.txt\n",
      "2015_fic.txt\n",
      "2015_mag.txt\n",
      "2015_news.txt\n",
      "2015_spok.txt\n",
      "2012_acad.txt\n",
      "2012_fic.txt\n",
      "2012_mag.txt\n",
      "2012_news.txt\n",
      "2012_spok.txt\n",
      "2013_acad.txt\n",
      "2013_fic.txt\n"
     ]
    }
   ],
   "source": [
    "coca_texts = {}\n",
    "for files in coca_raw:\n",
    "    # collect year and genre info\n",
    "    print(files)\n",
    "    if files[0] != \"w\":\n",
    "        year, genretxt = files.split(\"_\")\n",
    "        genre = genretxt[0:-4]\n",
    "    else:\n",
    "        w, genre, yeartxt = files.split(\"_\")\n",
    "        year = yeartxt[:4]\n",
    "    coca_text = clean_raw_text(coca_raw[files][1:])\n",
    "    for text_list in coca_text:\n",
    "        txts = lucem_illud_2020.word_tokenize(text_list)\n",
    "        try:\n",
    "            coca_texts[txts[0][2:]] = txts[1:]\n",
    "            \n",
    "            #append info to dataframe\n",
    "            text_df['year'].append(year)\n",
    "            text_df['genre'].append(genre)\n",
    "            text_df['text'].append(text_list)\n",
    "            text_df['tokenized_text'].append(txts[1:])\n",
    "            \n",
    "        except IndexError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus = pd.DataFrame.from_dict(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus.to_pickle(\"full_corpora.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus.to_csv(\"1990_2011_coca_corpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 5 year Epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus['Epoch'] = .\n",
    "coca_corpus['Epoch'] = np.where(coca_corpus['year'] < 1995, 1, \n",
    "                               np.where(coca_corpus['year'] < 2000, 2,\n",
    "                                   np.where(coca_corpus['year'] < 2005, 3, \n",
    "                                           np.where(coca_corpus['year'] < 2010, 4,\n",
    "                                                    np.where(coca_corpus['year'] < 2015, 5))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus = pd.read_csv(\"../week-6/1990_2011_coca_corpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics - Counting Words & Phrases (Week 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of times relevant words occur within the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize tokens? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the parts of speech associated with lesbian, gay, bisexual, transgender, queer and the adjectives associated with the nouns, in a small sample of the corpora from the early years (1990) compared to the later years (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounter(wordLst):\n",
    "    wordCounts = {}\n",
    "    for word in wordLst:\n",
    "        #We usually need to normalize the case\n",
    "        wLower = word.lower()\n",
    "        if wLower in wordCounts:\n",
    "            wordCounts[wLower] += 1\n",
    "        else:\n",
    "            wordCounts[wLower] = 1\n",
    "    #convert to DataFrame\n",
    "    countsForFrame = {'word' : [], 'count' : []}\n",
    "    for w, c in wordCounts.items():\n",
    "        countsForFrame['word'].append(w)\n",
    "        countsForFrame['count'].append(c)\n",
    "    return pandas.DataFrame(countsForFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a random sample of texts from 1990\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Word Embeddings (Week 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coca_corpus['tokenized_sents'] = coca_corpus['text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "coca_corpus['normalized_sents'] = coca_corpus['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s, lemma=False) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaW2V = gensim.models.word2vec.Word2Vec(coca_corpus['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Change over Time (Week 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, category, text_column_name='normalized_sents', sort = True, embeddings_raw={}):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    if len(embeddings_raw) == 0:\n",
    "        embeddings_raw = rawModels(df, category, text_column_name, sort)\n",
    "    cats = sorted(set(df[category]))\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawModels(df, category, text_column_name='normalized_sents', sort = True):\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF[text_column_name].sum())\n",
    "    return embeddings_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Models\n",
    "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'gay'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'queer'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'transgender'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'bisexual'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'lesbian'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch = rawModels(coca_corpus, 'Epoch', text_column_name='normalized sents')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, embedding in enumerate(rawEmbeddings_epoch):\n",
    "    model = rawEmbeddings_epoch[embedding]\n",
    "    name = \"embedding_epoch_\" + str(epoch)\n",
    "    model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_embeddings(address, kind):\n",
    "    rawEmbeddings = {}\n",
    "    for file in os.listdir(address):\n",
    "        if \"embedding_\"+kind in file:\n",
    "            e, kind_, kind_type = file.split(\"_\")\n",
    "            kind_type = eval(kind_type)\n",
    "            rawEmbeddings[kind_type] = Word2Vec.load(file)\n",
    "    return rawEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings_epoch_load = file_to_embeddings(\".\", \"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
